{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccde65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules Imported!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import scipy.linalg as la\n",
    "from math import sqrt\n",
    "from keras.datasets import mnist\n",
    "import pdb\n",
    "print(\"Modules Imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab6a699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (12665, 1, 784)\n",
      "Y_train: (12665,)\n",
      "X_test:  (2115, 1, 784)\n",
      "Y_test:  (2115,)\n"
     ]
    }
   ],
   "source": [
    "(full_train_X, full_train_y), (full_test_X, full_test_y) = mnist.load_data()\n",
    "train_X = []\n",
    "train_y = []\n",
    "test_X = []\n",
    "test_y = []\n",
    "\n",
    "for i in range(0, len(full_train_y)):\n",
    "    if full_train_y[i] == 0 or full_train_y[i] == 1:\n",
    "        train_X.append(full_train_X[i].reshape(1, 784)/255)\n",
    "        train_y.append(full_train_y[i])\n",
    "for i in range(0, len(full_test_y)):\n",
    "    if full_test_y[i] == 0 or full_test_y[i] == 1:\n",
    "        test_X.append(full_test_X[i].reshape(1, 784)/255)\n",
    "        test_y.append(full_test_y[i])\n",
    "\n",
    "train_X = np.array(train_X)\n",
    "train_y = np.array(train_y)\n",
    "test_X = np.array(test_X)\n",
    "test_y = np.array(test_y)\n",
    "print('X_train: ' + str(train_X.shape))\n",
    "print('Y_train: ' + str(train_y.shape))\n",
    "print('X_test:  '  + str(test_X.shape))\n",
    "print('Y_test:  '  + str(test_y.shape))\n",
    "\n",
    "d = 28*28 #dimension\n",
    "N = len(train_X)\n",
    "\n",
    "initial_weights = np.random.randn(d) / np.sqrt(d) #initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee2a628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(w, X, y, rho, loss):\n",
    "    # w are the initial weights, should be of shape (1, d) where d is dimensionality\n",
    "    # X is the X training data, should be of shape (N, d) where N is number of samples, d is dimensionality\n",
    "    # y is the y training data, should be of shape (N, 1) where N is the number of samples\n",
    "    # rho is the learning rate\n",
    "    # loss is an empty array\n",
    "    \n",
    "    ones = np.ones(y.shape) #make array of 1s to be used for addition and subtraction in sigmoid and loss/likelihood function\n",
    "    \n",
    "    sig = ones/(ones + np.exp(-1*np.dot(X, w.T))) #put weights into sigmoid function\n",
    "    train_loss = np.sum(y*np.log(ones/sig) + (ones-y)*(np.log(ones/(ones-sig)))) / N #compute training loss\n",
    "    loss.append(train_loss)\n",
    "    #print(\"Iteration 0 training loss: \" + str(train_loss)) #print the initial loss\n",
    "        \n",
    "    for i in range(1, 50): #30 iterations for learning\n",
    "        h = np.exp(-1*np.dot(X, w.T)) #e^(-w*X) ... shape: (N, 1)\n",
    "        y_shift = np.subtract(y, ones) #make y output have 0, -1 instead of 1, 0\n",
    "        z = y_shift + np.divide(h, np.add(ones, h)) #taking (y + h/(1+h)) part of the gradient of log likelihood ... shape: (N, 1)\n",
    "        gradL = np.dot(z.T, X) / N #compute the gradient of the loss function ... shape: (1, d)\n",
    "        w = w + rho*gradL #update the weights\n",
    "        \n",
    "        sig = ones/(ones + np.exp(-1*np.dot(X, w.T))) #put weights into sigmoid function\n",
    "        train_loss = np.sum(y*np.log(ones/sig) + (ones-y)*(np.log(ones/(ones-sig)))) / N #compute training loss\n",
    "        loss.append(train_loss)\n",
    "        #print(\"Iteration \" + str(i) + \" training loss: \" + str(train_loss))\n",
    "        \n",
    "    return w #return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8f42b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 training loss: 0.7438488792349337\n",
      "Iteration 1 training loss: 0.5880776465994997\n",
      "Iteration 2 training loss: 0.48756952727564706\n",
      "Iteration 3 training loss: 0.4155791362938052\n",
      "Iteration 4 training loss: 0.3614220569215988\n",
      "Iteration 5 training loss: 0.31945195345608407\n",
      "Iteration 6 training loss: 0.28616278806450307\n",
      "Iteration 7 training loss: 0.2592282130454376\n",
      "Iteration 8 training loss: 0.23705174976037313\n",
      "Iteration 9 training loss: 0.21851084049651626\n",
      "Iteration 10 training loss: 0.2027992041951292\n",
      "Iteration 11 training loss: 0.18932624522692237\n",
      "Iteration 12 training loss: 0.17765139598113985\n",
      "Iteration 13 training loss: 0.1674404216652415\n",
      "Iteration 14 training loss: 0.1584357996562328\n",
      "Iteration 15 training loss: 0.15043627878562976\n",
      "Iteration 16 training loss: 0.14328253030553692\n",
      "Iteration 17 training loss: 0.13684691025188622\n",
      "Iteration 18 training loss: 0.1310260426251582\n",
      "Iteration 19 training loss: 0.12573536863664475\n",
      "Iteration 20 training loss: 0.12090508692179182\n",
      "Iteration 21 training loss: 0.11647709184335599\n",
      "Iteration 22 training loss: 0.11240263755288481\n",
      "Iteration 23 training loss: 0.10864053640604436\n",
      "Iteration 24 training loss: 0.10515575543118469\n",
      "Iteration 25 training loss: 0.1019183125822312\n",
      "Iteration 26 training loss: 0.09890240109583315\n",
      "Iteration 27 training loss: 0.09608568908995452\n",
      "Iteration 28 training loss: 0.09344875501320052\n",
      "Iteration 29 training loss: 0.0909746293055541\n",
      "0.9981087470449173\n"
     ]
    }
   ],
   "source": [
    "w = initial_weights.reshape(1, -1)\n",
    "X = train_X.reshape(-1, d)\n",
    "y = train_y.reshape(-1, 1)\n",
    "rho = 0.05\n",
    "gd_train_loss = []\n",
    "w = grad_desc(w, X, y, rho, gd_train_loss)\n",
    "correct = 0\n",
    "for i in range(0, len(test_X)):\n",
    "    model_output = np.dot(w, test_X[i].T)\n",
    "    if model_output > 0 and test_y[i] == 1:\n",
    "        correct += 1\n",
    "    elif model_output < 0 and test_y[i] == 0:\n",
    "        correct += 1\n",
    "print(correct/len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee75143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_grad_desc(w, X, y, rho, loss):\n",
    "    # w are the initial weights, should be of shape (1, d) where d is dimensionality\n",
    "    # X is the X training data, should be of shape (N, d) where N is number of samples, d is dimensionality\n",
    "    # y is the y training data, should be of shape (N, 1) where N is the number of samples\n",
    "    # rho is the learning rate\n",
    "    # loss is an empty array\n",
    "    \n",
    "    ones = np.ones(y.shape) #make array of 1s to be used for addition and subtraction in sigmoid and loss/likelihood function\n",
    "    \n",
    "    sig = ones/(ones + np.exp(-1*np.dot(X, w.T))) #put weights into sigmoid function\n",
    "    train_loss = np.sum(y*np.log(ones/sig) + (ones-y)*(np.log(ones/(ones-sig)))) / N #compute training loss\n",
    "    loss.append(train_loss)\n",
    "    #print(\"Iteration 0 training loss: \" + str(train_loss)) #print the initial loss\n",
    "        \n",
    "    for i in range(1, 50): #30 iterations for learning\n",
    "        h = np.exp(-1*np.dot(X, w.T)) #e^(-w*X) ... shape: (N, 1)\n",
    "        y_shift = np.subtract(y, ones) #make y output have 0, -1 instead of 1, 0\n",
    "        z = y_shift + np.divide(h, np.add(ones, h)) #taking (y + h/(1+h)) part of the gradient of log likelihood ... shape: (N, 1)\n",
    "        G = X * z #gradient of the log likelihood ... shape: (N, d)\n",
    "        F = np.dot(G.T, G) / N #compute the Fisher info matrix ... shape: (d, d)\n",
    "        F = F + np.eye(X.shape[1])*0.0001 #regularize the fisher matrix to make it invertible\n",
    "        gradL = np.dot(z.T, X) / N #compute the gradient of the loss function ... shape: (1, d)\n",
    "        Ng = np.linalg.solve(F, gradL.T) #solve the natural gradient vector ... shape: (d, 1)\n",
    "        w = w + rho*Ng.T #update the weights\n",
    "        \n",
    "        sig = ones/(ones + np.exp(-1*np.dot(X, w.T))) #put weights into sigmoid function\n",
    "        train_loss = np.sum(y*np.log(ones/sig) + (ones-y)*(np.log(ones/(ones-sig)))) / N #compute training loss\n",
    "        loss.append(train_loss)\n",
    "        #print(\"Iteration \" + str(i) + \" training loss: \" + str(train_loss))\n",
    "        \n",
    "    return w #return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "512b2eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 training loss: 0.7438488792349337\n",
      "Iteration 1 training loss: 0.6969113130405071\n",
      "Iteration 2 training loss: 0.649999300779318\n",
      "Iteration 3 training loss: 0.6031237620453782\n",
      "Iteration 4 training loss: 0.5563018962998327\n",
      "Iteration 5 training loss: 0.5095602978681135\n",
      "Iteration 6 training loss: 0.462939709537183\n",
      "Iteration 7 training loss: 0.4165023984236513\n",
      "Iteration 8 training loss: 0.3703438111969102\n",
      "Iteration 9 training loss: 0.3246112918236863\n",
      "Iteration 10 training loss: 0.27953429113079414\n",
      "Iteration 11 training loss: 0.23547183771011104\n",
      "Iteration 12 training loss: 0.19297994419460707\n",
      "Iteration 13 training loss: 0.15288423825345462\n",
      "Iteration 14 training loss: 0.11630527395823234\n",
      "Iteration 15 training loss: 0.08454370244209908\n",
      "Iteration 16 training loss: 0.058744479238633344\n",
      "Iteration 17 training loss: 0.039416719282756835\n",
      "Iteration 18 training loss: 0.026136088394705895\n",
      "Iteration 19 training loss: 0.017645405633875597\n",
      "Iteration 20 training loss: 0.012416924747946766\n",
      "Iteration 21 training loss: 0.009195522678015478\n",
      "Iteration 22 training loss: 0.007163230927143236\n",
      "Iteration 23 training loss: 0.005822412317818709\n",
      "Iteration 24 training loss: 0.004869937813068775\n",
      "Iteration 25 training loss: 0.004160728345433173\n",
      "Iteration 26 training loss: 0.0036238592059274484\n",
      "Iteration 27 training loss: 0.0032101654060357294\n",
      "Iteration 28 training loss: 0.0028862528767788585\n",
      "Iteration 29 training loss: 0.0026266787076278755\n",
      "0.9990543735224586\n"
     ]
    }
   ],
   "source": [
    "w = initial_weights.reshape(1, -1)\n",
    "X = train_X.reshape(-1, d)\n",
    "y = train_y.reshape(-1, 1)\n",
    "rho = 0.05\n",
    "ngd_train_loss = []\n",
    "w = natural_grad_desc(w, X, y, rho, ngd_train_loss)\n",
    "correct = 0\n",
    "for i in range(0, len(test_X)):\n",
    "    model_output = np.dot(w, test_X[i].T)\n",
    "    if model_output > 0 and test_y[i] == 1:\n",
    "        correct += 1\n",
    "    elif model_output < 0 and test_y[i] == 0:\n",
    "        correct += 1\n",
    "print(correct/len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2208f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6331287  0.51200533 0.43083667 0.37150985 0.32626753 0.29079018\n",
      " 0.2623444  0.23909962 0.21978903 0.20351396 0.18962294 0.17763435\n",
      " 0.16718573 0.15799978 0.14986116 0.14260034 0.13608221 0.13019783\n",
      " 0.12485848 0.11999117 0.11553536 0.11144043 0.1076637  0.104169\n",
      " 0.10092543 0.09790643 0.0950891  0.09245355 0.08998241 0.08766049\n",
      " 0.08547441 0.08341236 0.08146385 0.07961956 0.07787117 0.07621122\n",
      " 0.07463302 0.07313053 0.07169831 0.07033142 0.06902538 0.06777611\n",
      " 0.0665799  0.06543336 0.06433336 0.06327707 0.06226185 0.06128529\n",
      " 0.06034516 0.05943941]\n",
      "[6.33128703e-01 5.86146395e-01 5.39205636e-01 4.92332607e-01\n",
      " 4.45568204e-01 3.98975795e-01 3.52653453e-01 3.06753593e-01\n",
      " 2.61515010e-01 2.17315351e-01 1.74754176e-01 1.34767311e-01\n",
      " 9.87204919e-02 6.83010088e-02 4.48628820e-02 2.84360714e-02\n",
      " 1.79423037e-02 1.16770667e-02 8.02080993e-03 5.94354947e-03\n",
      " 4.73954008e-03 3.96755922e-03 3.41281743e-03 2.98596002e-03\n",
      " 2.64699560e-03 2.37290385e-03 2.14733034e-03 1.95791538e-03\n",
      " 1.79563343e-03 1.65409662e-03 1.52891165e-03 1.41713031e-03\n",
      " 1.31675675e-03 1.22638738e-03 1.14498382e-03 1.07173267e-03\n",
      " 1.00593535e-03 9.46841634e-04 8.93590444e-04 8.45374008e-04\n",
      " 8.01521743e-04 7.61480501e-04 7.24777949e-04 6.91000068e-04\n",
      " 6.59788816e-04 6.30842900e-04 6.03910098e-04 5.78779127e-04\n",
      " 5.55276035e-04 5.33260299e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Experiment\n",
    "\n",
    "X = train_X.reshape(-1, d)\n",
    "y = train_y.reshape(-1, 1)\n",
    "rho = 0.05\n",
    "gd_losses = []\n",
    "ngd_losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    w = np.random.randn(d) / np.sqrt(d) #random weights\n",
    "    gd_w = w.reshape(1, -1)\n",
    "    ngd_w = w.reshape(1, -1)\n",
    "    gd_loss = []\n",
    "    ngd_loss = []\n",
    "    grad_desc(gd_w, X, y, rho, gd_loss)\n",
    "    natural_grad_desc(ngd_w, X, y, rho, ngd_loss)\n",
    "    gd_losses.append(gd_loss)\n",
    "    ngd_losses.append(ngd_loss)\n",
    "\n",
    "avg_gd_loss = np.mean(np.array(gd_losses), axis=0)\n",
    "avg_ngd_loss = np.mean(np.array(ngd_losses), axis=0)\n",
    "iteration = [i for i in range(0, 50)]\n",
    "#print(avg_gd_loss)\n",
    "#print(avg_ngd_loss)\n",
    "plt.plot(iteration, avg_gd_loss)\n",
    "plt.plot(iteration, avg_ngd_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211452e",
   "metadata": {},
   "source": [
    "The natural gradient descent (orange) converges faster than regular gradient descent (blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9f170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
